{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84731438",
   "metadata": {},
   "source": [
    "## <span style=\"color:#a4d4a3\">**Kalman Filter**</span>\n",
    "\n",
    "The Kalman Filter is a <span style=\"color:#ffa500\">**Bayes filter**</span> specifically designed for <span style=\"color:#ffa500\">**linear Gaussian**</span> estimation problems. Under these assumptions, the Kalman Filter provides an <span style=\"color:#ffa500\">**optimal solution**</span>:\n",
    "\n",
    "- <span style=\"color:#00703c\">**Linear motion and observation models**</span>.\n",
    "- <span style=\"color:#00703c\">**Zero-mean Gaussian noise**</span> in both the motion and sensor measurements.\n",
    "\n",
    "Everything involved in the Kalman Filter <span style=\"color:#ffa500\">**remains Gaussian**</span>, which is essential for its mathematical formulation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985b5c6",
   "metadata": {},
   "source": [
    "\n",
    "### üîî <span style=\"color:#a4d4a3\">**Gaussian Distributions**</span>\n",
    "\n",
    "A Gaussian (normal) distribution is defined by:\n",
    "\n",
    "$$\n",
    "p(x) = \\det(2\\pi\\Sigma)^{-\\frac{1}{2}}\\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)\n",
    "$$\n",
    "\n",
    "Two crucial properties of Gaussian distributions are <span style=\"color:#ffa500\">**marginalization**</span> and <span style=\"color:#ffa500\">**conditioning**</span>:\n",
    "\n",
    "<span style=\"color:#a4d4a3\">**Marginalization:**</span>\n",
    "\n",
    "Given the joint Gaussian distribution:\n",
    "\n",
    "$$\n",
    "p(x) = p\\left(\n",
    "\\begin{bmatrix}\n",
    "x_a \\\\[6pt]\n",
    "x_b\n",
    "\\end{bmatrix}\\right) = \\mathcal{N}\\left(\n",
    "\\begin{bmatrix}\n",
    "\\mu_a \\\\[6pt]\n",
    "\\mu_b\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "\\Sigma_{aa} & \\Sigma_{ab} \\\\[6pt]\n",
    "\\Sigma_{ba} & \\Sigma_{bb}\n",
    "\\end{bmatrix}\\right)\n",
    "$$\n",
    "\n",
    "The marginal distribution of $ x_a $ is:\n",
    "\n",
    "$$\n",
    "p(x_a) = \\int p(x_a, x_b)\\,dx_b = \\mathcal{N}(\\mu_a, \\Sigma_{aa})\n",
    "$$\n",
    "\n",
    "<span style=\"color:#a4d4a3\">**Conditioning:**</span>\n",
    "\n",
    "The conditional distribution of $ x_a $ given $ x_b $ is Gaussian:\n",
    "\n",
    "$$\n",
    "p(x_a|x_b) = \\frac{p(x_a, x_b)}{p(x_b)} = \\mathcal{N}(\\mu, \\Sigma)\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "\\mu = \\mu_a + \\Sigma_{ab}\\Sigma_{bb}^{-1}(x_b - \\mu_b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma = \\Sigma_{aa} - \\Sigma_{ab}\\Sigma_{bb}^{-1}\\Sigma_{ba}\n",
    "$$\n",
    "\n",
    "> üìù <span style=\"color:#0098ff\">**Note:**</span> <em>Computing the inverse of $ \\Sigma_{bb} $ can be computationally expensive. Additionally, if little is known about $ x_b $, the second term tends toward zero.</em>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff03df",
   "metadata": {},
   "source": [
    "\n",
    "### üìè <span style=\"color:#a4d4a3\">**Linear Models**</span>\n",
    "\n",
    "The Kalman Filter <span style=\"color:#ffa500\">**assumes linear models**</span> of the form:\n",
    "\n",
    "<span style=\"color:#a4d4a3\">**Linear Motion Model:**\n",
    "\n",
    "$$\n",
    "x_t = A_t x_{t-1} + B_t u_t + \\epsilon_t\n",
    "$$\n",
    "\n",
    "<span style=\"color:#a4d4a3\">**Linear Observation Model:**\n",
    "\n",
    "$$\n",
    "z_t = C_t x_t + \\delta_t\n",
    "$$\n",
    "\n",
    "These represent the <span style=\"color:#ffa500\">**mean models**</span> where the uncertainty is explicitly included through noise terms $ \\epsilon_t, \\delta_t $, both Gaussian distributed with covariance matrices $ R_t $ and $ Q_t $.\n",
    "\n",
    "<span style=\"color:#a4d4a3\">**Components of the Kalman Filter:**</span>\n",
    "\n",
    "- $ A_t $: State transition matrix (n√ón) describing state evolution without controls or noise.\n",
    "- $ B_t $: Control input matrix (n√ól) describing how controls influence the state.\n",
    "- $ C_t $: Observation matrix (k√ón) mapping the state space to observations.\n",
    "- $ \\epsilon_t, \\delta_t $: Gaussian noise terms for process and measurement uncertainties, respectively.\n",
    "\n",
    "\n",
    "Thus, the probability distributions for the motion and observation models are:\n",
    "\n",
    "<span style=\"color:#a4d4a3\">**Motion under Gaussian noise:**</span>\n",
    "\n",
    "$$\n",
    "p(x_t|u_t,x_{t-1}) = det(2 \\pi R_t )^{-\\frac{1}{2}} \\cdot \\exp\\left(-\\frac{1}{2}(x_t - A_t x_{t-1} - B_t u_t)^T R_t^{-1}(x_t - A_t x_{t-1} - B_t u_t)\\right)\n",
    "$$\n",
    "\n",
    "where $ R_t $ describes the noise of the motion.\n",
    "\n",
    "<span style=\"color:#a4d4a3\">**Measurement under Gaussian noise:**</span>\n",
    "\n",
    "$$\n",
    "p(z_t|x_t) = det(2 \\pi Q_t )^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}(z_t - C_t x_t)^T Q_t^{-1}(z_t - C_t x_t)\\right)\n",
    "$$\n",
    "\n",
    "where $ Q_t $ describes the noise of the measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc30d2",
   "metadata": {},
   "source": [
    "\n",
    "### üßÆ <span style=\"color:#a4d4a3\">**Kalman Filter Algorithm**</span>\n",
    "\n",
    "The Kalman Filter recursively computes the belief distribution in two main steps: a <span style=\"color:#ffa500\">**prediction step**</span> and a <span style=\"color:#ffa500\">**correction step**</span>.\n",
    "\n",
    "\n",
    "> <tt> <span style=\"color:#FF2DD1\">1.</span> <span style=\"color:#4D96FF\">def</span> **<span style=\"color:#6BCB77\">Kalman_Filter</span>($\\color{#ffa500}\\mu_{t-1}, \\color{#ffa500}\\Sigma_{t-1}, \\color{#ffa500}u_t, \\color{#ffa500}z_t$):**\n",
    ">> <span style=\"color:#948979\"># Prediction step</span>   \n",
    ">><span style=\"color:#FF2DD1\">2.</span> $ \\bar{\\mu}_t = A_t \\mu_{t-1} + B_t u_t $  \n",
    ">><span style=\"color:#FF2DD1\">3.</span> $ \\bar{\\Sigma}_t = A_t \\Sigma_{t-1} A_t^T + R_t $ \n",
    ">>\n",
    ">><span style=\"color:#948979\"># Correction step</span>    \n",
    ">><span style=\"color:#FF2DD1\">3.</span> $K_t = \\bar{\\Sigma}_t C_t^T (C_t \\bar{\\Sigma}_t C_t^T + Q_t)^{-1} \\quad$ \n",
    ">><span style=\"color:#948979\"> # Compute Kalman Gain </span>     \n",
    ">> <span style=\"color:#FF2DD1\">4.</span> $\\mu_t = \\bar{\\mu}_t + K_t (z_t - C_t \\bar{\\mu}_t)\\quad\\quad$ \n",
    ">><span style=\"color:#948979\"> # Update mean </span>     \n",
    ">><span style=\"color:#FF2DD1\">5.</span> $\\Sigma_t = (I - K_t C_t)\\bar{\\Sigma}_t \\quad\\quad\\quad$ \n",
    ">><span style=\"color:#948979\"> # Update covariance </span>    \n",
    ">>\n",
    ">> <span style=\"color:#948979\"># Updated belief</span>    \n",
    ">><span style=\"color:#FF2DD1\">6.</span> return $ \\mu_t, \\Sigma_t $      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e3024",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style=\"color:#a4d4a3\">Interpretation of the Kalman Gain</span> ($K_t$)\n",
    "\n",
    "The Kalman gain trades off how certain we are about the observations compared to the motion prediction:\n",
    "\n",
    "- <span style=\"color:#ffa500\">**No measurement uncertainty**</span> ($Q_t = 0$): $\\quad K_t = C_t^{-1}$\n",
    "\n",
    "The update directly maps observations into state space.\n",
    "\n",
    "- <span style=\"color:#ffa500\">**Infinite measurement uncertainty**</span> ($Q_t = \\infty$): $\\quad K_t = 0$\n",
    "\n",
    "No correction step is performed.\n",
    "\n",
    "> üìñ <span style=\"color:#0098ff\">**Further Reading:**</span> <em>Mathematical proofs and detailed derivations of the Kalman Filter can be found in *Probabilistic Robotics*, Section 3.2.4.</em>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61c2fb",
   "metadata": {},
   "source": [
    "##### üìà `Python Example #1: 1D Kalman Filter`\n",
    "\n",
    "In this section we walk through a simple 1-dimensional Kalman Filter example: <span style=\"color:#ffa500\">**Prediction**</span>, <span style=\"color:#ffa500\">**Measurement**</span>, and <span style=\"color:#ffa500\">**Correction**</span>.\n",
    "\n",
    "1. We use the motion model  \n",
    "$$\n",
    "x_t = A\\,x_{t-1} + B\\,u + \\epsilon,\\quad \\epsilon\\sim\\mathcal{N}(0,R)\n",
    "$$  \n",
    "\n",
    "to compute the <span style=\"color:#ffa500\">**predicted**</span> mean and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prior belief\n",
    "mu_0, sigma_0 = 0.0, 1.0\n",
    "\n",
    "# Motion model parameters\n",
    "A, B = 1.0, 1.0\n",
    "u = 5.0\n",
    "R = 2.0\n",
    "\n",
    "# Prediction\n",
    "mu_pred    = A * mu_0 + B * u\n",
    "sigma_pred = A**2 * sigma_0 + R\n",
    "\n",
    "# Plot Prediction\n",
    "x = np.linspace(-5, 15, 400)\n",
    "pred_pdf = (1/np.sqrt(2*np.pi*sigma_pred)) \\\n",
    "           * np.exp(-0.5*(x-mu_pred)**2/sigma_pred)\n",
    "\n",
    "plt.figure(figsize=(4, 2))  # Adjust figure size\n",
    "plt.plot(x, pred_pdf, 'r--', label='Prediction')\n",
    "plt.title('Prediction')\n",
    "plt.xlim(x.min(), x.max())\n",
    "plt.ylim(0, pred_pdf.max()*1.1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82af6fe",
   "metadata": {},
   "source": [
    "2. Then, we simulate a noisy measurement\n",
    "\n",
    "$$\n",
    "\n",
    "z = C_t x_t + \\delta, \\quad \\delta \\sim \\mathcal{N}(0, Q)\n",
    "\n",
    "$$\n",
    "\n",
    "and plot it against our prior prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51636a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurement model parameters\n",
    "C = 1.0\n",
    "Q = 4.0\n",
    "z = 8.0\n",
    "\n",
    "# Measurement distribution\n",
    "meas_pdf = (1/np.sqrt(2*np.pi*Q)) \\\n",
    "           * np.exp(-0.5*(x-z)**2/Q)\n",
    "\n",
    "# Plot Measurement + Prediction\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.plot(x, pred_pdf, 'r--', label='Prediction')\n",
    "plt.plot(x, meas_pdf, 'g-',  label='Measurement')\n",
    "plt.title('Measurement')\n",
    "plt.xlim(x.min(), x.max())\n",
    "plt.ylim(0, max(pred_pdf.max(), meas_pdf.max())*1.1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea7f17",
   "metadata": {},
   "source": [
    "3. Finally, we update our belief with the Kalman gain\n",
    "\n",
    "$$\n",
    "K = \\frac{\\sigma_{pred} C}{C^2 \\sigma_{pred} + Q}\n",
    "$$\n",
    "\n",
    "to get the <span style=\"color:#ffa500\">**corrected**</span> mean and variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction step\n",
    "K        = sigma_pred * C / (C**2 * sigma_pred + Q)\n",
    "mu_corr  = mu_pred + K * (z - C * mu_pred)\n",
    "sigma_corr = (1 - K * C) * sigma_pred\n",
    "\n",
    "# Correction distribution\n",
    "corr_pdf = (1/np.sqrt(2*np.pi*sigma_corr)) \\\n",
    "           * np.exp(-0.5*(x-mu_corr)**2/sigma_corr)\n",
    "\n",
    "# Plot Correction + Prediction + Measurement\n",
    "plt.figure(figsize=(4, 2))  # Adjusted figure size\n",
    "plt.plot(x, pred_pdf, 'r--', label='Prediction')\n",
    "plt.plot(x, meas_pdf, 'g-',  label='Measurement')\n",
    "plt.plot(x, corr_pdf, 'm-',  label='Correction')\n",
    "plt.title('Correction')\n",
    "plt.xlim(x.min(), x.max())\n",
    "plt.ylim(0, max(pred_pdf.max(), meas_pdf.max(), corr_pdf.max())*1.1)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61392ed",
   "metadata": {},
   "source": [
    "This illustrates the core idea of Kalman filtering: <span style=\"color:#ffa500\">**recursive fusion**</span> of prediction and observation under <span style=\"color:#ffa500\">**Gaussian assumptions**</span>. In higher-dimensional SLAM problems, the same principles apply, only the matrices grow, but the filter remains an <span style=\"color:#ffa500\">**optimal, efficient estimator**</span> for <span style=\"color:#ffa500\">**linear Gaussian systems**</span>. \n",
    "\n",
    "> üßê <span style=\"color:#0098ff\">***But how often do we see linear systems in the real-world and especially in robotic systems where rotations and angles are involved?***</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d53a5",
   "metadata": {},
   "source": [
    "## <span style=\"color:#a4d4a3\">**Extended Kalman Filter (EKF)**</span>\n",
    "\n",
    "While the Kalman Filter is optimal and efficient for linear Gaussian systems, most realistic robotics problems involve <span style=\"color:#ffa500\">**non-linear dynamic systems**</span>. \n",
    "\n",
    "For example, consider a robot performing **rotations** in a 2D plane. The rotation dynamics are inherently non-linear due to trigonometric functions that govern the orientation of the robot. The previously linear state-space equations:\n",
    "\n",
    "$$\n",
    "x_t = A_t x_{t-1} + B_t u_t + \\epsilon_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_t = C_t x_t + \\delta_t\n",
    "$$\n",
    "\n",
    "no longer adequately describe such systems. Instead, we use more general non-linear functions:\n",
    "\n",
    "$$\n",
    "x_t = g(u_t, x_{t-1}) + \\epsilon_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_t = h(x_t) + \\delta_t\n",
    "$$\n",
    "\n",
    "where $g$ might represent the robot's motion model involving angles and rotations, and $h$ could represent sensor measurements (e.g., range and bearing sensors).\n",
    "\n",
    "These non-linear functions break the Gaussian assumptions of the standard Kalman filter. Therefore, the filter is no longer directly applicable, and we must resort to <span style=\"color:#ffa500\">**local approximations**</span>, leading us naturally to the <span style=\"color:#ffa500\">**Extended Kalman Filter (EKF)**</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eacd066",
   "metadata": {},
   "source": [
    "\n",
    "### üë®üèº‚Äçüíª <span style=\"color:#a4d4a3\">**EKF Linearization** (First-Order Taylor Expansion)</span>\n",
    "\n",
    "The EKF <span style=\"color:#ffa500\">**approximates**</span> the non-linear motion and sensor models using a first-order Taylor expansion around the current estimated state:\n",
    "\n",
    "<span style=\"color:#a4d4a3\">**Prediction Linearization**:</span>\n",
    "\n",
    "$$\n",
    "g(u_t, x_{t-1}) \\approx g(u_t, \\mu_{t-1}) + \\frac{\\partial g(u_t, \\mu_{t-1})}{\\partial x_{t-1}} (x_{t-1} - \\mu_{t-1})\n",
    "$$\n",
    "\n",
    "We define the Jacobian matrix as:\n",
    "\n",
    "$$\n",
    "G_t = \\frac{\\partial g(u_t, \\mu_{t-1})}{\\partial x_{t-1}}\n",
    "$$\n",
    "\n",
    "<span style=\"color:#a4d4a3\">**Correction Linearization**:</span>\n",
    "\n",
    "$$\n",
    "h(x_t) \\approx h(\\bar{\\mu}_t) + \\frac{\\partial h(\\bar{\\mu}_t)}{\\partial x_t}(x_t - \\bar{\\mu}_t)\n",
    "$$\n",
    "\n",
    "We define the Jacobian matrix as:\n",
    "\n",
    "$$\n",
    "H_t = \\frac{\\partial h(\\bar{\\mu}_t)}{\\partial x_t}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaabc14",
   "metadata": {},
   "source": [
    "\n",
    "##### <span style=\"color:#a4d4a3\">Linearized Motion Model</span>\n",
    "\n",
    "The linearized motion model under Gaussian noise is expressed as:\n",
    "\n",
    "$$\n",
    "p(x_t|u_t, x_{t-1}) \\approx \\det(2\\pi R_t)^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}(x_t - g(u_t,\\mu_{t-1}) - G_t(x_{t-1}-\\mu_{t-1}))^T R_t^{-1}(x_t - g(u_t,\\mu_{t-1}) - G_t(x_{t-1}-\\mu_{t-1}))\\right)\n",
    "$$\n",
    "\n",
    "##### <span style=\"color:#a4d4a3\">Linearized Sensor Model</span>\n",
    "\n",
    "Similarly, the linearized sensor model under Gaussian noise is given by:\n",
    "\n",
    "$$\n",
    "p(z_t|x_t) \\approx \\det(2\\pi Q_t)^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}(z_t - h(\\bar{\\mu}_t) - H_t(x_t - \\bar{\\mu}_t))^T Q_t^{-1}(z_t - h(\\bar{\\mu}_t) - H_t(x_t - \\bar{\\mu}_t))\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf7b8f",
   "metadata": {},
   "source": [
    "\n",
    "### üßÆ <span style=\"color:#a4d4a3\">**Extended Kalman Filter Algorithm**</span>\n",
    "\n",
    "The EKF follows a similar structure to the Kalman Filter, but with the nonlinear functions and their linearizations:\n",
    "\n",
    "> <tt> <span style=\"color:#FF2DD1\">1.</span> <span style=\"color:#4D96FF\">def</span> **<span style=\"color:#6BCB77\">Extended_Kalman_Filter</span>($\\color{#ffa500}\\mu_{t-1}, \\color{#ffa500}\\Sigma_{t-1}, \\color{#ffa500}u_t, \\color{#ffa500}z_t$):**\n",
    ">> <span style=\"color:#948979\"># Prediction step</span>   \n",
    ">><span style=\"color:#FF2DD1\">2.</span> $ \\bar{\\mu}_t = g(u_t, \\mu_{t-1})\\quad$<span style=\"color:#948979\"> # Compute predicted mean </span>     \n",
    ">><span style=\"color:#FF2DD1\">3.</span> $ \\bar{\\Sigma}_t = G_t \\Sigma_{t-1} G_t^T + R_t\\quad $ <span style=\"color:#948979\"> # Compute predicted covariance </span>      \n",
    ">>\n",
    ">><span style=\"color:#948979\"># Correction step</span>    \n",
    ">><span style=\"color:#FF2DD1\">3.</span> $K_t = \\bar{\\Sigma}_t H_t^T(H_t \\bar{\\Sigma}_t H_t^T + Q_t)^{-1}\\quad$ \n",
    ">><span style=\"color:#948979\"> # Compute Kalman Gain </span>     \n",
    ">> <span style=\"color:#FF2DD1\">4.</span> $\\mu_t = \\bar{\\mu}_t + K_t(z_t - h(\\bar{\\mu}_t))\\quad$ \n",
    ">><span style=\"color:#948979\"> # Correct the mean estimate </span>  \n",
    ">><span style=\"color:#FF2DD1\">5.</span> $\\Sigma_t = (I - K_t H_t)\\bar{\\Sigma}_t\\quad\\quad$ \n",
    ">><span style=\"color:#948979\"> # Correct the covariance estimate </span>    \n",
    ">>\n",
    ">><span style=\"color:#948979\"># Updated belief</span>    \n",
    ">><span style=\"color:#FF2DD1\">6.</span> return $\\mu_t, \\Sigma_t$    \n",
    "</tt>\n",
    "\n",
    "The EKF approach allows us to apply Bayesian filtering to realistic robotic systems involving <span style=\"color:#ffa500\">**non-linear dynamics**</span> and sensor models. However, the quality of estimation now heavily depends on the accuracy of the <span style=\"color:#ffa500\">**local linear approximations**</span>.\n",
    "\n",
    "> üìù <span style=\"color:#0098ff\">**Note**</span>: <em> In EKF, $A_t$ and $C_t$ from the standard Kalman filter are replaced with the Jacobian matrices $G_t$ and $H_t$, respectively.</em>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ddd70",
   "metadata": {},
   "source": [
    "##### üìà `Python Example #2: Linear vs. Non-linear Mapping`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98868d8e",
   "metadata": {},
   "source": [
    "<span style=\"color:#a4d4a3\">**1. Gaussian distribution through a Linear Map**</span>\n",
    "\n",
    "We start with a simple case to illustrate a fundamental property of Gaussian distributions:  \n",
    "\n",
    "- when passed through a <span style=\"color:#ffa500\">**linear function**</span>, the result remains <span style=\"color:#ffa500\">**Gaussian**</span>.\n",
    "\n",
    "Let the prior distribution $ p(x) \\sim \\mathcal{N}(\\mu=0, \\sigma=0.5) $.  \n",
    "\n",
    "We apply a linear transformation:\n",
    "\n",
    "$$\n",
    "y = -0.5x + 1\n",
    "$$\n",
    "\n",
    "The resulting distribution $ p(y) $ is also Gaussian, with transformed mean and variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear mapping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Prior ---\n",
    "x = np.linspace(-2, 2, 400)\n",
    "mu, sigma = 0.0, 0.5\n",
    "prior_pdf = norm.pdf(x, mu, sigma)\n",
    "\n",
    "# --- Linear mapping y = -0.5x + 1 ---\n",
    "a, b = -0.5, 1.0\n",
    "y_lin = a * x + b\n",
    "mu_lin = a * mu + b  # mean of the output distribution\n",
    "sigma_lin = abs(a) * sigma  # scaled standard deviation (|a| since variance scales with a^2)\n",
    "y_grid = np.linspace(mu_lin - 3.5*sigma_lin, mu_lin + 3.5*sigma_lin, 400)  # evaluation range set to cover ¬±3.5œÉ around the new mean\n",
    "pdf_lin = norm.pdf(y_grid, mu_lin, sigma_lin)   # the resulting probability density function p(y), which remains Gaussian.\n",
    "\n",
    "# --- Plot ---\n",
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "axs[0].plot(x, prior_pdf, 'k-')\n",
    "axs[0].set_title('Prior $p(x)$')\n",
    "axs[1].plot(x, y_lin, 'b-')\n",
    "axs[1].set_title('$y = -0.5x + 1$')\n",
    "axs[2].plot(y_grid, pdf_lin, 'b-')\n",
    "axs[2].fill_between(y_grid, 0, pdf_lin, color='b', alpha=0.15)\n",
    "\n",
    "axs[0].set_xlim(-2, 2); axs[0].set_ylim(0, 2.0)\n",
    "axs[1].set_xlim(-2, 2); axs[1].set_ylim(0, 2.0)\n",
    "axs[2].set_xlim(-2, 2); axs[2].set_ylim(0, 2.0)\n",
    "axs[2].set_title('Gaussian $p(y)$')\n",
    "for ax in axs: ax.grid(True)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bada0808",
   "metadata": {},
   "source": [
    "<span style=\"color:#a4d4a3\">**2. Gaussian distribution through a Non-linear Map**</span>\n",
    "\n",
    "In contrast to the linear case, we now apply a <span style=\"color:#ffa500\">**non-linear transformation**</span> to the same Gaussian prior:\n",
    "\n",
    "$$\n",
    "y = \\sin(x) + 0.1 \\cdot \\sin(5x)\n",
    "$$\n",
    "\n",
    "This function introduces local curvature and high-frequency variation, which distorts the Gaussian shape of the distribution. The output distribution $ p(y) $ is no longer Gaussian.\n",
    "\n",
    "We use <span style=\"color:#ffa500\">**Monte Carlo sampling**</span> to estimate how the distribution transforms.\n",
    "\n",
    "<details>\n",
    "<summary><strong>üìù <span style=\"color:#e74c3c\">Note:</span></strong> <em> Monte Carlo Push-Forward </em> </summary>\n",
    "<br>\n",
    "\n",
    "Since the non-linear function $ f(x) = \\sin(x) + 0.1\\sin(5x) $ is too complex for analytical treatment, we rely on a <span style=\"color:#ffa500\">**Monte Carlo method**</span> to approximate the resulting distribution $ p(y) $.\n",
    "\n",
    "That is, we:\n",
    "\n",
    "1. <span style=\"color:#00703c\">**Sample**</span> many values $ x \\sim \\mathcal{N}(\\mu, \\sigma) $.\n",
    "2. <span style=\"color:#00703c\">**Transform**</span> each sample using $ y = f(x) $.\n",
    "3. <span style=\"color:#00703c\">**Aggregate**</span> the transformed values into a histogram.\n",
    "\n",
    "This gives a data-driven approximation of the true push-forward distribution, which captures all non-linear distortions introduced by $ f(x) $.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ce756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-linear mapping\n",
    "def f_nl(x): return np.sin(x) + 0.1*np.sin(5*x)\n",
    "\n",
    "# Monte-Carlo push-forward\n",
    "N = 100_000\n",
    "ys  = f_nl(np.random.normal(mu, sigma, N))  # apply f_nl to random samples from the Gaussian prior p(x).\n",
    "y_nl = np.linspace(ys.min(), ys.max(), 500)\n",
    "pdf_nl, _ = np.histogram(ys, bins=500, range=(ys.min(), ys.max()), density=True)  # estimate the resulting distribution p(y) using a histogram\n",
    "\n",
    "# --- Plot ---\n",
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3))  # Adjusted figure size\n",
    "axs[0].plot(x, prior_pdf, 'k-')\n",
    "axs[0].set_title('Prior $p(x)$')\n",
    "axs[1].plot(x, f_nl(x), 'r-')\n",
    "axs[1].set_title('$y = \\\\sin x + 0.1\\\\sin 5x$')\n",
    "axs[2].plot(y_nl, pdf_nl, 'r-')\n",
    "axs[2].fill_between(y_nl, 0, pdf_nl, color='r', alpha=0.15)\n",
    "axs[2].set_title('Non-Gaussian $p(y)$')\n",
    "\n",
    "axs[0].set_xlim(-2, 2); axs[0].set_ylim(0, 2)\n",
    "axs[1].set_xlim(-2, 2); axs[1].set_ylim(-2, 2)\n",
    "axs[2].set_xlim(-2, 2); axs[2].set_ylim(0, 2)\n",
    "\n",
    "for ax in axs: ax.grid(True)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c831e",
   "metadata": {},
   "source": [
    "<span style=\"color:#a4d4a3\">**3. Local Linearization**</span>\n",
    "\n",
    "To address the distortion introduced by the non-linear map, the <span style=\"color:#ffa500\">**Extended Kalman Filter (EKF)**</span> performs a <span style=\"color:#ffa500\">**local linear approximation**</span> of the function around a point $ x_0 $. This allows it to maintain a Gaussian belief while capturing local non-linear behavior.\n",
    "\n",
    "We linearize the function  \n",
    "$$\n",
    "f(x) = \\sin(x) + 0.1\\sin(5x)\n",
    "$$  \n",
    "around $ x_0 = 0.4 $ using a first-order Taylor expansion:\n",
    "\n",
    "$$\n",
    "f(x) \\approx f(x_0) + f'(x_0)(x - x_0)\n",
    "$$\n",
    "\n",
    "The EKF then pushes the Gaussian prior through this linear approximation. This results in an approximate Gaussian distribution $ p(y) $, centered at $ f(x_0) $, with variance scaled by the local slope $ f'(x_0) $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f40f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local linearization\n",
    "x0 = 0.4    # point of linearization, move around to see different effects\n",
    "G0 = np.cos(x0) + 0.1*5*np.cos(5*x0)          # local slope: derivative f'(x0)\n",
    "y0 = f_nl(x0)    # output of the function at x0\n",
    "\n",
    "# Local linear function around x0\n",
    "dx = 0.5\n",
    "x_local = np.linspace(x0-dx, x0+dx, 100)\n",
    "y_local = y0 + G0*(x_local - x0)    # linearized version of the function in a local window\n",
    "\n",
    "# Push-forward Gaussian\n",
    "sigma_ekf = abs(G0)*sigma\n",
    "y_ekf = np.linspace(y0 - 3.5*sigma_ekf, y0 + 3.5*sigma_ekf, 400)\n",
    "pdf_ekf = norm.pdf(y_ekf, y0, sigma_ekf)    # Gaussian approximation of the push-forward using first-order Taylor expansion\n",
    "\n",
    "# --- Plot ---\n",
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "\n",
    "# Prior\n",
    "axs[0].plot(x, prior_pdf, 'k-')\n",
    "axs[0].set_title('Prior $p(x)$')\n",
    "\n",
    "# Mapping + linearization\n",
    "axs[1].plot(x, f_nl(x), 'r-', label='$f(x)$')\n",
    "axs[1].plot(x_local, y_local, 'm--', label='Linearization')\n",
    "axs[1].plot(x0, y0, 'ko', label='$x_0=0.4$')\n",
    "axs[1].set_title('Local linearization')\n",
    "axs[1].legend()\n",
    "\n",
    "# Push-forward comparison\n",
    "axs[2].plot(y_nl, pdf_nl, 'r-', alpha=0.3, label='True $p(y)$')\n",
    "axs[2].plot(y_ekf, pdf_ekf, 'm--', label='Gaussian approx.')\n",
    "axs[2].fill_between(y_ekf, 0, pdf_ekf, color='m', alpha=0.15)\n",
    "axs[2].set_title('Linear approximation')\n",
    "axs[2].legend()\n",
    "\n",
    "axs[0].set_xlim(-2, 2); axs[0].set_ylim(0, 2.0)\n",
    "axs[1].set_xlim(-2, 2); axs[1].set_ylim(-2, 2)\n",
    "axs[2].set_xlim(-2, 2); axs[2].set_ylim(0, 2.0)\n",
    "\n",
    "for ax in axs: ax.grid(True)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68359cf0",
   "metadata": {},
   "source": [
    "#### ‚úîÔ∏è <span style=\"color:#a4d4a3\">Conclusion: Why Local Linearization Matters</span>\n",
    "\n",
    "This example illustrates the <span style=\"color:#ffa500\">**limitations of the standard Kalman Filter**</span> and the <span style=\"color:#ffa500\">**motivation behind the Extended Kalman Filter (EKF)**</span>.\n",
    "\n",
    "- A <span style=\"color:#ffa500\">**linear transformation**</span> of a Gaussian distribution results in another Gaussian ‚Äî this is the core strength of the standard Kalman Filter.\n",
    "- However, real-world (especially robotic) systems often involve <span style=\"color:#ffa500\">**non-linear models**</span>, where the output distribution becomes <span style=\"color:#ffa500\">**non-Gaussian**</span>, potentially multi-modal or skewed.\n",
    "- The EKF provides a compromise: it <span style=\"color:#ffa500\">**locally linearizes**</span> the non-linear model around the current mean. This allows the filter to maintain a <span style=\"color:#ffa500\">**Gaussian belief**</span> while still capturing <span style=\"color:#ffa500\">**local curvature**</span> in the dynamics or observations.\n",
    "\n",
    "> ‚ö†Ô∏è <span style=\"color:#0098ff\">**Note:**</span> <em> EKF does <span style=\"color:#e74c3c\">**not**</span> produce exact results for non-linear systems. But in many practical scenarios where the system is <span style=\"color:#e74c3c\">**\"locally close to linear\"**</span>, it provides a good approximation while remaining computationally efficient.</em>\n",
    "\n",
    "This lays the foundation for applying EKF to <span style=\"color:#ffa500\">**non-linear motion models**</span> and <span style=\"color:#ffa500\">**sensor models**</span> in SLAM, which we will explore next.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9326e248",
   "metadata": {},
   "source": [
    "### üìö <span style=\"color:#a4d4a3\">**Reading Material**</span>\n",
    "\n",
    "**Kalman Filter and EKF**\n",
    "- Thrun et al.: *\"Probabilistic Robotics\"*, **Chapter 3**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
