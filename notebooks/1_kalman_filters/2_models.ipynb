{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0daa8278",
   "metadata": {},
   "source": [
    "## <span style=\"color:#a4d4a3\"> **Motion and Observation Models** </span>\n",
    "\n",
    "Below we detail common motion and observation models used in SLAM and provide a few `python` examples.\n",
    "\n",
    "---\n",
    "\n",
    "### ü™É <span style=\"color:#a4d4a3\"> **Motion Models** </span>\n",
    "\n",
    "In robotics, the <span style=\"color:#ffa500\">**motion model**</span> describes how the robot state evolves from one time step to the next, given a previous state and a control input. Specifically, it provides the posterior probability that a control action $u_t$ moves the robot from state $x_{t-1}$ to state $x_t$:\n",
    "\n",
    "$$\n",
    "p(x_t \\mid u_t, x_{t-1})\n",
    "$$\n",
    "\n",
    "As previously discussed in the context of the Bayes Filter, we need a way to explicitly model the uncertainty in robot motion, as real-world motion always involves <span style=\"color:#ffa500\">**inherent uncertainty**</span>.\n",
    "\n",
    "Two common motion models used in practice are:\n",
    "\n",
    "1. <span style=\"color:#00703c\">**Odometry-based Motion Model**</span> (typically used for wheeled robots).\n",
    "2. <span style=\"color:#00703c\">**Velocity-based Motion Model**</span> (often employed for aerial or legged robots).\n",
    "\n",
    "#### <span style=\"color:#a4d4a3\"> 1. Odometry-based Motion Model (for wheeled robots) </span>\n",
    "\n",
    "The odometry model relies on wheel encoders, which count wheel rotations to estimate robot displacement. However, factors like tire pressure differences, wheel slippage, or uneven terrain can introduce drift over time. Thus, odometry measurements inherently contain uncertainty.\n",
    "\n",
    " <span style=\"color:#a4d4a3\">**Odometry Measurements:**</span>\n",
    "\n",
    "Given the previous robot pose $(\\bar{x}, \\bar{y}, \\bar{\\theta})$ and the new robot pose after movement $(\\bar{x}', \\bar{y}', \\bar{\\theta}')$, we define the odometry measurement $u$ as a vector of rotational and translational changes:\n",
    "\n",
    "$$\n",
    "u = (\\delta_{rot1}, \\delta_{trans}, \\delta_{rot2})^T\n",
    "$$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../figures/odom_model.png\" alt=\"Robot Odometry\" width=\"640\"/>\n",
    "</p>\n",
    "\n",
    "where each component is calculated as follows:\n",
    "\n",
    "- **Translational displacement**:\n",
    "$$\n",
    "\\delta_{trans} = \\sqrt{(\\bar{x}' - \\bar{x})^2 + (\\bar{y}' - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "- **Initial rotation** (to face the direction of motion):\n",
    "$$\n",
    "\\delta_{rot1} = \\text{atan2}(\\bar{y}' - \\bar{y},\\, \\bar{x}' - \\bar{x}) - \\bar{\\theta}\n",
    "$$\n",
    "\n",
    "- **Final rotation** (to reach the final orientation):\n",
    "$$\n",
    "\\delta_{rot2} = \\bar{\\theta}' - \\bar{\\theta} - \\delta_{rot1}\n",
    "$$\n",
    "\n",
    "In other words, the motion is decomposed into three sequential steps:\n",
    "\n",
    "1. An initial rotation ($\\delta_{rot1}$) aligning the robot toward the target position.\n",
    "2. A translational motion ($\\delta_{trans}$) moving the robot forward to the new location.\n",
    "3. A final rotation ($\\delta_{rot2}$) aligning the robot with its desired orientation.\n",
    "\n",
    " <span style=\"color:#a4d4a3\">**Modeling Uncertainty:**</span>\n",
    "\n",
    "Due to inherent noise, uncertainty in these measurements is typically modeled as <span style=\"color:#ffa500\">**additive Gaussian noise**</span> in each odometry measurement:\n",
    "\n",
    "$$\n",
    "u_{noisy} = u + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\Sigma)\n",
    "$$\n",
    "\n",
    "where $\\Sigma$ is the covariance matrix representing uncertainty in rotations and translation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff9ca5e",
   "metadata": {},
   "source": [
    "##### üìà `Python Example #1: Odometry-based Motion Model`\n",
    "\n",
    "First, we define the <span style=\"color:#ffa500\">**odometry-based motion model**</span> according to the equations provided earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74662e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample motion model for a robot using odometry data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed for reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "# --- Motion model ---\n",
    "def motion_model_odometry(u:np.array, x_prev:np.array, noise_std:np.array=[0, 0, 0]) -> np.array:\n",
    "    \"\"\"\n",
    "    Simulates the robot's motion given odometry commands and previous state.\n",
    "    Inputs:\n",
    "    - u: Odometry command (delta_rot1, delta_trans, delta_rot2)\n",
    "    - x_prev: Previous state (x, y, theta)\n",
    "    - noise_std: Standard deviations for Gaussian noise added\n",
    "    Returns:\n",
    "    - x: New state (x, y, theta)\n",
    "    \"\"\"\n",
    "    delta_rot1, delta_trans, delta_rot2 = u\n",
    "    \n",
    "    # Add Gaussian noise to odometry commands\n",
    "    delta_rot1_hat = delta_rot1 + np.random.normal(0, noise_std[0])\n",
    "    delta_trans_hat = delta_trans + np.random.normal(0, noise_std[1])\n",
    "    delta_rot2_hat = delta_rot2 + np.random.normal(0, noise_std[2])\n",
    "    \n",
    "    # Compute new state\n",
    "    x = x_prev[0] + delta_trans_hat * np.cos(x_prev[2] + delta_rot1_hat)\n",
    "    y = x_prev[1] + delta_trans_hat * np.sin(x_prev[2] + delta_rot1_hat)\n",
    "    theta = x_prev[2] + delta_rot1_hat + delta_rot2_hat\n",
    "    \n",
    "    return np.array([x, y, theta])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57006ef",
   "metadata": {},
   "source": [
    "Then, we define an initial position and a set of desired odometry commands $ u $. We feed these commands to the motion model to generate trajectories for two cases:\n",
    "\n",
    "- One <span style=\"color:#ffa500\">**without noise**</span>, representing the <span style=\"color:#ffa500\">**ideal**</span> (ground truth) trajectory.\n",
    "- One <span style=\"color:#ffa500\">**with Gaussian noise**</span>, representing <span style=\"color:#ffa500\">**realistic**</span> conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aacb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial state [x, y, theta]\n",
    "x_start = np.array([0, 0, 0])\n",
    "\n",
    "# Odometry commands [rot1, trans, rot2], you can play with these values\n",
    "commands = [\n",
    "    [np.pi/4, 1.0, np.pi/4],\n",
    "    [0, 1.5, 0],\n",
    "    [-np.pi/2, 2.0, -np.pi/4],\n",
    "    [np.pi/2, 1.0, 0]\n",
    "]\n",
    "\n",
    "# Ground truth trajectory (noise-free)\n",
    "trajectory_gt = [x_start]\n",
    "\n",
    "# Noisy trajectory (with odometry noise)\n",
    "trajectory_noisy = [x_start]\n",
    "\n",
    "x_gt = x_start.copy()\n",
    "x_noisy = x_start.copy()\n",
    "\n",
    "# Iterate over commands and update trajectories\n",
    "for u in commands:\n",
    "    # Ground truth update (no noise)\n",
    "    x_gt = motion_model_odometry(u, x_gt)\n",
    "    trajectory_gt.append(x_gt)\n",
    "    \n",
    "    # Noisy update\n",
    "    x_noisy = motion_model_odometry(u, x_noisy, noise_std=[0.06, 0.1, 0.06])\n",
    "    trajectory_noisy.append(x_noisy)\n",
    "\n",
    "trajectory_gt = np.array(trajectory_gt)\n",
    "trajectory_noisy = np.array(trajectory_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57fc6ee",
   "metadata": {},
   "source": [
    "Below, we visualize both trajectories. The noisy trajectory clearly demonstrates how uncertainty from odometry can cause drift over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the trajectories\n",
    "plt.figure(figsize=(4, 4)) # Adjust figure size for better visibility\n",
    "plt.plot(trajectory_gt[:,0], trajectory_gt[:,1], 'o-', label='Ground Truth', linewidth=2)\n",
    "plt.plot(trajectory_noisy[:,0], trajectory_noisy[:,1], 'x--', label='Odometry (with noise)', linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.title('Odometry-based Motion Model Example')\n",
    "plt.xlabel('X position [m]')\n",
    "plt.ylabel('Y position [m]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa5bb6",
   "metadata": {},
   "source": [
    "Additionally, we illustrate how the uncertainty in each odometry command (rotation-only, translation-only, and combined) affects the resulting distribution of poses in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff198b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of samples from the motion model with different noise scenarios\n",
    "# Initial pose and odometry command (straight motion)\n",
    "initial_pose = np.array([0.0, 0.0, 0.0])\n",
    "odometry_command = [0.2, 2.0, 0.0]  # initial heading, translate 2 m, no rotation\n",
    "\n",
    "# Define noise scenarios using the same noise_std signature\n",
    "scenarios = {\n",
    "    'Translation Only': [0.0, 0.04, 0.0],\n",
    "    'Rotation Only':    [0.04, 0.0, 0.04],\n",
    "    'Combined':         [0.04, 0.04, 0.04]\n",
    "}\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3)) # Adjust figure size for better visibility\n",
    "\n",
    "for ax, (title, noise) in zip(axes, scenarios.items()):\n",
    "    # Generate samples by repeatedly calling the single-sample function\n",
    "    samples = np.array([motion_model_odometry(odometry_command, initial_pose, noise) for _ in range(1000)])\n",
    "    ax.scatter(samples[:, 0], samples[:, 1], s=10, alpha=0.6, label='Samples')\n",
    "    ax.plot(initial_pose[0], initial_pose[1], 'ro', label='Initial Pose')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X position [m]')\n",
    "    if ax is axes[0]:\n",
    "        ax.set_ylabel('Y position [m]')\n",
    "    ax.axis('equal')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270dd56e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c438d2",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style=\"color:#a4d4a3\"> 2. Velocity-based Motion Model </span>\n",
    "\n",
    "Typically, a velocity-based motion model is expressed through two velocities: (i) a translational velocity and (ii) a rotational velocity. This results in an <span style=\"color:#ffa500\">**arc-like trajectory**</span> for the robot's motion.\n",
    "\n",
    "In this model, we define:\n",
    "\n",
    "- Translational velocity: $v$\n",
    "- Rotational velocity: $w$\n",
    "\n",
    "Given the current robot state $(x, y, \\theta)$ and the control input vector $u = (v, w)^T$, the updated robot state $(x', y', \\theta')$ after applying these controls for a time interval $\\Delta t$ is computed as follows:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../figures/velocity_model.png\" alt=\"Robot Odometry\" width=\"520\"/>\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x' \\\\[6pt]\n",
    "y' \\\\[6pt]\n",
    "\\theta'\n",
    "\\end{pmatrix} \n",
    "=\n",
    "\\begin{pmatrix}\n",
    "x \\\\[6pt]\n",
    "y \\\\[6pt]\n",
    "\\theta\n",
    "\\end{pmatrix} \n",
    "+\n",
    "\\begin{pmatrix}\n",
    "-\\frac{v}{w}\\sin\\theta + \\frac{v}{w}\\sin(\\theta + w \\Delta t) \\\\[6pt]\n",
    "\\frac{v}{w}\\cos\\theta - \\frac{v}{w}\\cos(\\theta + w \\Delta t) \\\\[6pt]\n",
    "w \\Delta t + \\gamma \\Delta t\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "> üìù <span style=\"color:#0098ff\">**Note:**</span> <em>Arc-like motion alone doesn't allow specification of a particular final orientation. Therefore, we often add an extra term $ \\gamma \\Delta t $ to adjust the orientation explicitly. </em>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fa497",
   "metadata": {},
   "source": [
    "##### üìà `Python Example #2: Velocity-based Motion Model`\n",
    "\n",
    "First, we define the <span style=\"color:#ffa500\">**velocity-based motion model**</span> according to the equations provided earlier. This model uses three control inputs: (i) translational velocity $ v $, (ii) rotational velocity $ w $, and (iii) an additional term $ \\gamma $ for adjusting the orientation explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample motion model using velocity commands\n",
    "\n",
    "# --- Motion model ---\n",
    "def motion_model_velocity(u:np.array, x_prev:np.array, dt:float=1.0, noise_std:np.array=[0, 0, 0]) -> np.array:\n",
    "    \"\"\"\n",
    "    Sample velocity-based motion model particles.\n",
    "    Inputs:\n",
    "    - u: control commands [v, w, gamma]\n",
    "    - x_prev: previous state [x, y, theta]\n",
    "    - dt: time interval\n",
    "    - noise_std: standard deviations for [v, w, gamma] noise\n",
    "    Returns:\n",
    "    - x: new state [x', y', theta']\n",
    "    \"\"\"\n",
    "    v, w, gamma = u\n",
    "    \n",
    "    # Add Gaussian noise to controls\n",
    "    v_hat = v + np.random.normal(0, noise_std[0])\n",
    "    w_hat = w + np.random.normal(0, noise_std[1])\n",
    "    gamma_hat = gamma + np.random.normal(0, noise_std[2])\n",
    "    \n",
    "    # Compute new state with velocity model\n",
    "    if abs(w_hat) > 1e-6:\n",
    "        x_new = x_prev[0] - (v_hat/w_hat) * np.sin(x_prev[2]) + (v_hat/w_hat) * np.sin(x_prev[2] + w_hat*dt)\n",
    "        y_new = x_prev[1] + (v_hat/w_hat) * np.cos(x_prev[2]) - (v_hat/w_hat) * np.cos(x_prev[2] + w_hat*dt)\n",
    "    else:\n",
    "        # Straight line when w is zero\n",
    "        x_new = x_prev[0] + v_hat * dt * np.cos(x_prev[2])\n",
    "        y_new = x_prev[1] + v_hat * dt * np.sin(x_prev[2])\n",
    "    theta_new = x_prev[2] + w_hat * dt + gamma_hat * dt\n",
    "\n",
    "    return np.array([x_new, y_new, theta_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596a7741",
   "metadata": {},
   "source": [
    "Then, we define an initial position and a set of desired velocity commands $ u = [v, w, \\gamma]^T $. By sequentially applying these commands, we generate two trajectories:\n",
    "\n",
    "- One **without noise**: the ideal trajectory (ground truth).\n",
    "- One **with Gaussian noise**: simulating realistic conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e072f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial state\n",
    "initial_pose = np.array([0.0, 0.0, 0.0])\n",
    "control = [1.0, 0.3, 0.1]  # v, w, # Define a sequence of varied controls to create a non-circular path\n",
    "control_sequence = [\n",
    "    [1.0,  0.0,  0.0],  # straight\n",
    "    [1.0,  0.5,  0.0],  # turn right\n",
    "    [1.0,  0.0,  0.0],  # straight\n",
    "    [1.0, -0.5,  0.0],  # turn left\n",
    "    [1.0,  0.0,  0.0],  # straight\n",
    "    [1.0,  0.5,  0.0],  # turn right again\n",
    "    [1.0,  0.0,  0.0],  # straight\n",
    "    [1.0, -0.5,  0.0],  # turn left again\n",
    "    [1.0,  0.0,  0.2],  # adjust orientation\n",
    "    [1.0,  0.0, -0.2]   # adjust orientation back\n",
    "]\n",
    "\n",
    "initial_pose = np.array([0.0, 0.0, 0.0])\n",
    "num_steps = len(control_sequence)\n",
    "\n",
    "# Allocate trajectories\n",
    "traj_gt = np.zeros((num_steps+1, 3))\n",
    "traj_noisy = np.zeros((num_steps+1, 3))\n",
    "traj_gt[0] = initial_pose\n",
    "traj_noisy[0] = initial_pose\n",
    "\n",
    "# Simulate\n",
    "for t, u in enumerate(control_sequence, start=1):\n",
    "    traj_gt[t] = motion_model_velocity(u, traj_gt[t-1], dt=1.0)\n",
    "    traj_noisy[t] = motion_model_velocity(u, traj_noisy[t-1], dt=1.0, noise_std=[0.05,0.075,0.05])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42359f0d",
   "metadata": {},
   "source": [
    "Below, we visualize both trajectories. The noisy trajectory illustrates how uncertainty in velocity commands causes the robot‚Äôs actual trajectory to drift from the ideal path, especially as errors accumulate over multiple steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the trajectories\n",
    "plt.figure(figsize=(6, 3))  # Adjust figure size for better visibility\n",
    "plt.plot(traj_gt[:,0], traj_gt[:,1], 'o-', label='Ground Truth (no noise)', linewidth=2)\n",
    "plt.plot(traj_noisy[:,0], traj_noisy[:,1], 'x--', label='Noisy Trajectory', linewidth=1)\n",
    "plt.scatter(initial_pose[0], initial_pose[1], c='red', s=100, label='Start')\n",
    "plt.title('Velocity-based Motion Model: Trajectories')\n",
    "plt.xlabel('X position [m]')\n",
    "plt.ylabel('Y position [m]')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72d3ec",
   "metadata": {},
   "source": [
    "To further understand the impact of uncertainty, we also visualize the effects of noise applied independently to each velocity command ‚Äî translational ($ v $), rotational ($ w $), and orientation adjustment ($ \\gamma $) ‚Äî and their combined effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a5ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two distinct commands and a combined one\n",
    "initial_pose = np.array([0.0, 0.0, 0.0])\n",
    "control = [2.0, 0.5, 0.2]  # v, w, gamma\n",
    "num_samples = 1000\n",
    "\n",
    "# Scenarios: noise on one control at a time\n",
    "scenarios = {\n",
    "    'v Only':      [0.05, 0.00, 0.00],\n",
    "    'w Only':      [0.00, 0.075, 0.00],\n",
    "    'both v and w': [0.05, 0.075, 0.00]\n",
    "} # Play with these values to see different effects\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))  # Adjust figure size for better visibility\n",
    "\n",
    "for ax, (title, noise) in zip(axes, scenarios.items()):\n",
    "    # Sample particles\n",
    "    particles = np.array([\n",
    "        motion_model_velocity(control, initial_pose, dt=1.0, noise_std=noise)\n",
    "        for _ in range(num_samples)\n",
    "    ])\n",
    "    # Plot\n",
    "    ax.scatter(particles[:,0], particles[:,1], s=10, alpha=0.6, label='Samples')\n",
    "    ax.plot(initial_pose[0], initial_pose[1], 'ro', linestyle='None', label='Initial Pose')\n",
    "    \n",
    "    ax.set_title(f'Noise in {title}')\n",
    "    ax.set_xlabel('X position [m]')\n",
    "    if ax is axes[0]:\n",
    "        ax.set_ylabel('Y position [m]')\n",
    "    ax.axis('equal')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf93bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b37262",
   "metadata": {},
   "source": [
    "##### üïπÔ∏è `Python example #3: Interactive Odometry Motion Model`\n",
    "\n",
    "In this example, we‚Äôll build an interactive demonstration of the <span style=\"color:#ffa500\">**odometry-based motion model** </span>. You can use the arrow keys to drive the robot, and you‚Äôll see:\n",
    "\n",
    "- <span style=\"color:#00703c\"> **Left panel (Ground Truth):** </span>  \n",
    "  The true robot pose updates exactly as you command (no noise).\n",
    "\n",
    "- <span style=\"color:#00703c\"> **Right panel (Noisy):** </span>  \n",
    "  The actual executed trajectory that includes the inevitable noise we typically encounter in real-world systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3939ed",
   "metadata": {},
   "source": [
    "Initialize `pygame` window parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "import pygame\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "# Panel and window sizes\n",
    "PANEL_SIZE = (600, 400)  # Adjust panel size as needed\n",
    "WINDOW_SIZE = (PANEL_SIZE[0] * 2, PANEL_SIZE[1])\n",
    "\n",
    "clock = pygame.time.Clock()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926c5f2",
   "metadata": {},
   "source": [
    "Set initial robot poses at center; store trajectory histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- State ---\n",
    "# Ground truth and noisy poses: [x, y, theta]\n",
    "# Initialize both poses at the center of the panel\n",
    "gt_pose    = np.array([PANEL_SIZE[0]//2, PANEL_SIZE[1]//2, 0.0])\n",
    "noisy_pose = gt_pose.copy()\n",
    "\n",
    "# Trajectory histories\n",
    "gt_traj    = [gt_pose.copy()]\n",
    "noisy_traj = [noisy_pose.copy()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654af34a",
   "metadata": {},
   "source": [
    "Define odometry noise and movement increments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf55ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Parameters ---\n",
    "# Odometry noise std: [rot1, trans, rot2]\n",
    "noise_std = [0.05, 0.075, 0.05]  # radians, pixels, radians\n",
    "\n",
    "# Command magnitudes\n",
    "TRANS_STEP = 20                   # pixels forward\n",
    "ROT_STEP   = np.deg2rad(15)       # radians rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c5458",
   "metadata": {},
   "source": [
    "We apply the noise on the two rotations and one translation on the odometry motion model we defined earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57dc384",
   "metadata": {},
   "source": [
    "Each frame (capped at 10 FPS), we clear the screen, draw the two panel borders, and handle events. On an <span style=\"color:#ffa500\">**arrow‚Äêkey**</span> press we compute a control vector `u` and then update both the ground‚Äêtruth pose (without noise) and the noisy pose (with sampled odometry noise), appending each to its trajectory history. Finally, we render the ground‚Äêtruth path as blue lines and a triangular robot, and the noisy path as yellow lines and a circular marker, before flipping the display. The loop continues until the window is closed, at which point we exit `pygame` cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = pygame.display.set_mode(WINDOW_SIZE)\n",
    "pygame.display.set_caption(\"Interactive Odometry Motion Model\")\n",
    "\n",
    "# --- Main Loop ---\n",
    "running = True\n",
    "while running:\n",
    "    # Cap to 10 FPS\n",
    "    clock.tick(10)\n",
    "    \n",
    "    # Clear screen\n",
    "    screen.fill((30, 30, 30))\n",
    "    \n",
    "    # Draw panel boundaries\n",
    "    pygame.draw.rect(screen, (200, 200, 200), (0, 0, *PANEL_SIZE), 2)\n",
    "    pygame.draw.rect(screen, (200, 200, 200), (PANEL_SIZE[0], 0, *PANEL_SIZE), 2)\n",
    "    pygame.draw.line(screen, (200, 200, 200), (PANEL_SIZE[0], 0), (PANEL_SIZE[0], PANEL_SIZE[1]), 2)\n",
    "    \n",
    "    # Handle input\n",
    "    for ev in pygame.event.get():\n",
    "        if ev.type == pygame.QUIT:\n",
    "            running = False\n",
    "        elif ev.type == pygame.KEYDOWN:\n",
    "            # Define command u = [rot1, trans, rot2]\n",
    "            if ev.key == pygame.K_UP:\n",
    "                u = [0.0, TRANS_STEP, 0.0]\n",
    "            elif ev.key == pygame.K_LEFT:\n",
    "                # turn left by negative angle to match intuitive arrow direction\n",
    "                u = [-ROT_STEP, 0.0, 0.0]\n",
    "            elif ev.key == pygame.K_RIGHT:\n",
    "                # turn right by positive angle\n",
    "                u = [ROT_STEP, 0.0, 0.0]\n",
    "            else:\n",
    "                u = None\n",
    "            \n",
    "            if u is not None:\n",
    "                # 1) Update ground truth (no noise)\n",
    "                gt_pose = motion_model_odometry(u, gt_pose, noise_std=[0,0,0])\n",
    "                gt_traj.append(gt_pose.copy())\n",
    "                \n",
    "                # 2) Update noisy pose\n",
    "                noisy_pose = motion_model_odometry(u, noisy_pose, noise_std)\n",
    "                noisy_traj.append(noisy_pose.copy())\n",
    "    \n",
    "    # --- Draw Left Panel: Ground Truth ---\n",
    "    for i in range(1, len(gt_traj)):\n",
    "        x0, y0, _ = gt_traj[i-1]\n",
    "        x1, y1, _ = gt_traj[i]\n",
    "        pygame.draw.line(screen, (0, 150, 255), (x0, y0), (x1, y1), 2)\n",
    "    # Draw ground truth robot as a triangle\n",
    "    gt_x, gt_y, gt_th = gt_pose\n",
    "    pts = [\n",
    "        (gt_x + 15*np.cos(gt_th), gt_y + 15*np.sin(gt_th)),\n",
    "        (gt_x + 10*np.cos(gt_th + 2.5), gt_y + 10*np.sin(gt_th + 2.5)),\n",
    "        (gt_x + 10*np.cos(gt_th - 2.5), gt_y + 10*np.sin(gt_th - 2.5))\n",
    "    ]\n",
    "    pygame.draw.polygon(screen, (0, 150, 255), pts)\n",
    "    \n",
    "    # --- Draw Right Panel: Noisy Trajectory ---\n",
    "    offset = PANEL_SIZE[0]\n",
    "    # Draw noisy trajectory\n",
    "    for i in range(1, len(noisy_traj)):\n",
    "        x0, y0, _ = noisy_traj[i-1]\n",
    "        x1, y1, _ = noisy_traj[i]\n",
    "        pygame.draw.line(screen, (200, 200, 60), (x0 + offset, y0), (x1 + offset, y1), 2)\n",
    "    # Draw noisy robot pose as a circle\n",
    "    np_x, np_y, _ = noisy_pose\n",
    "    pygame.draw.circle(screen, (200, 200, 60), (int(np_x) + offset, int(np_y)), 8)\n",
    "    \n",
    "    pygame.display.flip()\n",
    "\n",
    "pygame.quit()\n",
    "sys.exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d3975",
   "metadata": {},
   "source": [
    "#### ‚úîÔ∏è <span style=\"color:#a4d4a3\">Conclusion</span>\n",
    "\n",
    "This interactive example demonstrates a key concept in probabilistic robotics: <span style=\"color:#ffa500\">**the motion we command is not always the motion we get**</span>. While the ground truth robot follows inputs exactly, the noisy trajectory highlights how real-world factors, like imperfect actuators, wheel slip, and sensor inaccuracies, cause deviation from expected motion. This visualizes why probabilistic motion models are essential in SLAM and robotic state estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8729572",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8707050",
   "metadata": {},
   "source": [
    "### üî≠ <span style=\"color:#a4d4a3\">**Observation (Sensor) Models**</span>\n",
    "\n",
    "An <span style=\"color:#ffa500\">**observation model**</span> relates the robot's state to sensor measurements. It specifies the likelihood that a measurement $ z_t $ is obtained given the current state $ x_t $:\n",
    "\n",
    "$$\n",
    "p(z_t \\mid x_t)\n",
    "$$\n",
    "\n",
    "As sensor measurements inherently contain noise and uncertainty, the observation model must explicitly account for this uncertainty.\n",
    "\n",
    "Below, we present a simple example of a common observation model: the <span style=\"color:#ffa500\">**Range-Bearing Observation Model**</span> for landmark perception.\n",
    "\n",
    "\n",
    "#### <span style=\"color:#a4d4a3\">Range-Bearing Sensor Model (Landmark-based)</span>\n",
    "\n",
    "Consider a sensor such as a 2D LiDAR or radar that measures landmarks by providing:\n",
    "\n",
    "- <span style=\"color:#00703c\">**Range**:</span> distance from the robot to the landmark.\n",
    "- <span style=\"color:#00703c\">**Bearing**:</span> angle from the robot's orientation to the landmark.\n",
    "\n",
    "Given:\n",
    "\n",
    "- The robot‚Äôs current state: $ x_t = [x, y, \\theta]^T $\n",
    "- A landmark‚Äôs known position: $ m_j = [m_{j,x}, m_{j,y}]^T $\n",
    "\n",
    "The observation measurement $ z_t^j $ to landmark $ j $ is described by:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../figures/range_bearing_model.png\" alt=\"Robot Odometry\" width=\"420\"/>\n",
    "</p>\n",
    "\n",
    "$$\n",
    "z_t^j = \n",
    "\\begin{bmatrix}\n",
    "r_j \\\\[6pt]\n",
    "\\varphi_j\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sqrt{(m_{j,x} - x)^2 + (m_{j,y} - y)^2} \\\\[6pt]\n",
    "\\text{atan2}(m_{j,y} - y, m_{j,x} - x) - \\theta\n",
    "\\end{bmatrix}\n",
    "+ q\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ r_j $ is the measured distance (range) to the landmark.\n",
    "- $ \\varphi_j $ is the measured angle (bearing) relative to the robot‚Äôs orientation.\n",
    "- $ q $ represents the measurement noise, typically modeled as Gaussian noise:\n",
    "\n",
    "$$\n",
    "q \\sim \\mathcal{N}(0, Q)\n",
    "$$\n",
    "\n",
    "with covariance matrix:\n",
    "\n",
    "$$\n",
    "Q = \n",
    "\\begin{bmatrix}\n",
    "\\sigma_{range}^2 & 0 \\\\[6pt]\n",
    "0 & \\sigma_{bearing}^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a478ab",
   "metadata": {},
   "source": [
    "##### üìà`Python example #4: Range-bearing Sensor Model`\n",
    "\n",
    "First, we define the <span style=\"color:#ffa500\">**range-bearing sensor model**</span> according to the equations introduced earlier. We will simulate sensor measurements to landmarks, clearly demonstrating the difference between ideal measurements (noise-free) and realistic sensor measurements affected by Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Range-bearing sensor model ---\n",
    "def range_bearing_observation(x:np.array, landmark:np.array, noise_std:np.array=[0, 0]) -> np.array:\n",
    "    \"\"\"\n",
    "    2D range-bearing sensor model with Gaussian noise.\n",
    "    Inputs:\n",
    "    - x: robot state [x, y, theta]\n",
    "    - landmark: position [lx, ly]\n",
    "    - noise_std: measurement noise standard deviations [range, bearing]\n",
    "    Returns:\n",
    "    - measurement [range, bearing]\n",
    "    \"\"\"\n",
    "    dx = landmark[0] - x[0]\n",
    "    dy = landmark[1] - x[1]\n",
    "    \n",
    "    # Ideal measurements\n",
    "    r = np.sqrt(dx**2 + dy**2)\n",
    "    phi = np.arctan2(dy, dx) - x[2]\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    r_noisy = r + np.random.normal(0, noise_std[0])\n",
    "    phi_noisy = phi + np.random.normal(0, noise_std[1])\n",
    "    \n",
    "    return np.array([r_noisy, phi_noisy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f98d5",
   "metadata": {},
   "source": [
    "Next, we initialize a known position for the robot and several landmark positions. Using our defined range-bearing sensor model, we simulate <span style=\"color:#ffa500\">**sensor measurements**</span> to these landmarks, explicitly incorporating sensor noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Robot state and landmark positions\n",
    "robot_pose = np.array([2.0, 3.0, np.pi/4])\n",
    "landmarks = np.array([[5, 7], [1, 8], [7, 2]])\n",
    "\n",
    "# Simulate noisy measurements\n",
    "measurements = np.array([range_bearing_observation(robot_pose, lm, [0.1, 0.05]) for lm in landmarks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da55a7",
   "metadata": {},
   "source": [
    "Finally, we visualize the scenario by plotting the robot position, the ground truth landmark positions, and the noisy sensor measurements. This plot illustrates how sensor noise influences the accuracy of landmark observations and helps us understand the importance of probabilistic filtering methods in robotics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot robot, landmarks, and measurements\n",
    "plt.figure(figsize=(3, 3))  # Adjust figure size for better visibility\n",
    "plt.plot(robot_pose[0], robot_pose[1], 'bo', label='Robot')\n",
    "plt.quiver(robot_pose[0], robot_pose[1], np.cos(robot_pose[2]), np.sin(robot_pose[2]), \n",
    "           color='blue', angles='xy', scale_units='xy', scale=1.0, width=0.01)\n",
    "\n",
    "for idx, lm in enumerate(landmarks):\n",
    "    plt.plot(lm[0], lm[1], 'r*', markersize=10, label='Landmark' if idx==0 else \"\")\n",
    "    \n",
    "    # Ideal measurement lines\n",
    "    plt.plot([robot_pose[0], lm[0]], [robot_pose[1], lm[1]], 'k--', linewidth=0.5)\n",
    "    \n",
    "    # Noisy measurement lines (range-bearing)\n",
    "    r, phi = measurements[idx]\n",
    "    lm_noisy_x = robot_pose[0] + r * np.cos(robot_pose[2] + phi)\n",
    "    lm_noisy_y = robot_pose[1] + r * np.sin(robot_pose[2] + phi)\n",
    "    plt.plot([robot_pose[0], lm_noisy_x], [robot_pose[1], lm_noisy_y], 'g-', linewidth=2, alpha=0.6, label='Measurement' if idx==0 else \"\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('X position [m]')\n",
    "plt.ylabel('Y position [m]')\n",
    "plt.title('Range-Bearing Observation Model')\n",
    "plt.axis('equal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253efbc",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong> üìù <span style=\"color:#e74c3c\">Note:</span> </strong> <em>What is the practical purpose of the observation model equations since we do not actually know the positions of the landmarks?</em> </summary>\n",
    "<br>\n",
    "\n",
    "In practice, the robot's sensors (like LiDAR or radar) only provide raw measurements of <span style=\"color:#ffa500\">**range**</span> (distance) and <span style=\"color:#ffa500\">**bearing**</span> (angle) relative to the robot's position and orientation. The landmark coordinates are not directly given by the sensor.\n",
    "\n",
    "However, the <span style=\"color:#ffa500\">**range-bearing observation model**</span> equations allow us to compute what measurements we <span style=\"color:#ffa500\">**expect**</span> to see from a hypothesized robot position and orientation to a landmark whose position we estimate. \n",
    "\n",
    "These predicted measurements are crucial because we compare them with the actual sensor measurements. By quantifying differences between predicted and actual measurements, we iteratively update and refine our estimates of both the robot‚Äôs pose and landmark positions. This comparison process is fundamental to SLAM and localization, enabling robots to accurately map their environment and determine their location, even though real-world sensors provide noisy measurements and initial landmark positions are uncertain or unknown.\n",
    "\n",
    "We will later see in detail how to incorporate these motion and observation models into the filtering algorithms (e.g., Kalman Filter, Particle Filter, Graph‚Äêbased SLAM) to perform state estimation.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4f918",
   "metadata": {},
   "source": [
    "##### üïπÔ∏è `Python Example #5: Interactive Observation Model`\n",
    "\n",
    "\n",
    "In this example, we'll create an interactive visualization of a <span style=\"color:#ffa500\">**range-bearing sensor model**</span> using `pygame`. We will load a black-and-white image representing a simple floor plan. The visualization is split into two sections:\n",
    "\n",
    "- <span style=\"color:#00703c\">**Left panel:**</span> Displays the floor plan and simulates our rotating sensor (represented visually by a rotating beam).\n",
    "- <span style=\"color:#00703c\">**Right panel:**</span> Shows sensor measurements (landmark observations) as they are detected by the sensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b60c6",
   "metadata": {},
   "source": [
    "We start by defining the essential parameters of our simulated sensor, including:\n",
    "\n",
    "- <span style=\"color:#ffa500\">**Maximum measurement range**</span> (in pixels), limiting how far the sensor beam can detect obstacles.\n",
    "- <span style=\"color:#ffa500\">**Rotation frequency**</span> of the sensor beam, which simulates how quickly our sensor completes a full rotation (here set to 10Hz).\n",
    "- <span style=\"color:#ffa500\">**Measurement noise parameters**</span>, specifying Gaussian noise added to both the measured range (distance) and bearing (angle) to realistically represent sensor inaccuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script visualizes a floor plan and occupancy map using Pygame\n",
    "import pygame\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Beam parameters\n",
    "max_range   = 200\n",
    "beam_speed  = 2 * np.pi * 10  # 10 rev/s\n",
    "beam_angle  = 0.0\n",
    "\n",
    "# Measurement noise std [px, rad]\n",
    "noise_std = [2.0, np.deg2rad(0.5)]\n",
    "\n",
    "# Persistence settings\n",
    "landmark_duration_ms = 1_000_000 # landmark alive time in milliseconds\n",
    "registered = []  # (x,y,time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4546cfd3",
   "metadata": {},
   "source": [
    "We load the floor plan and build the <em>\"occupancy\"</em> map based on the pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc43d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & scale floor plan\n",
    "floor_plan_orig = pygame.image.load('../figures/floor_plan.png')  # You can replace this with any black-and-white floor plan image\n",
    "w_orig, h_orig = floor_plan_orig.get_size()\n",
    "max_w, max_h = 500, 400   # half-size for clarity\n",
    "scale = min(max_w / w_orig, max_h / h_orig)\n",
    "w, h = int(w_orig * scale), int(h_orig * scale)\n",
    "\n",
    "floor_plan = pygame.transform.smoothscale(floor_plan_orig, (w, h))\n",
    "\n",
    "# Build occupancy map (True = wall)‚å®Ô∏è\n",
    "arr = pygame.surfarray.array3d(floor_plan)\n",
    "wall_mask = np.all(arr < 128, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f656e",
   "metadata": {},
   "source": [
    "We then define a function, `cast_beam`, which simulates the sensor‚Äôs operation. Given the robot‚Äôs current pixel coordinates (controlled by the mouse), the current angle of the sensor beam, and the maximum sensor range, this function returns information about whether the beam hits an obstacle (black pixel) and, if so, at what distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cast a beam ---\n",
    "def cast_beam(pos:np.array, angle:float, max_r:float, noise_std:float=0.0) -> tuple:\n",
    "    \"\"\"\n",
    "    Cast a beam from pos at angle up to max_r.\n",
    "    Inputs:\n",
    "    - pos: (x, y) starting position\n",
    "    - angle: direction in radians\n",
    "    - max_r: maximum range\n",
    "    Returns (hit_pos, distance) or (None, max_r) if no hit.\n",
    "    \"\"\"\n",
    "    x0, y0 = pos\n",
    "    for d in range(int(max_r)):\n",
    "        x = int(x0 + d * np.cos(angle))\n",
    "        y = int(y0 + d * np.sin(angle))\n",
    "        if x<0 or x>=w or y<0 or y>=h:\n",
    "            return None, d\n",
    "        if wall_mask[x, y]:\n",
    "            return (x, y), d+np.random.normal(0, noise_std)\n",
    "    return None, int(max_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f74572",
   "metadata": {},
   "source": [
    "Finally, we run the simulation in a continuous loop, visually updating the scene using `pygame`. Detected landmarks are registered and persistently displayed in the right-hand panel, showing how sensor measurements accumulate over time. \n",
    "\n",
    "üëâ <span style=\"color:#00703c\">**To interact:**</span>\n",
    "\n",
    "Move your mouse cursor around the floor plan in the left panel. As you do, observe how landmarks appear on the right side, representing noisy sensor measurements detected by your simulated sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54398255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Display setup: two panels side-by-side\n",
    "screen = pygame.display.set_mode((w*2, h))\n",
    "pygame.display.set_caption(\"Map & Sensor View\")\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Main loop\n",
    "running = True\n",
    "while running:\n",
    "    dt = clock.tick(60) / 1000.0\n",
    "    now = pygame.time.get_ticks()\n",
    "    beam_angle = (beam_angle + beam_speed * dt) % (2 * np.pi)\n",
    "\n",
    "    for ev in pygame.event.get():\n",
    "        if ev.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "    # Clear screen\n",
    "    screen.fill((50, 50, 50))\n",
    "\n",
    "    # Left panel: Map + beam\n",
    "    screen.blit(floor_plan, (0, 0))\n",
    "    robot = np.array(pygame.mouse.get_pos())\n",
    "    robot = robot.clip([0,0],[w-1,h-1])\n",
    "    pygame.draw.circle(screen, (0,0,255), robot, 5)\n",
    "\n",
    "    hit, dist = cast_beam(robot, beam_angle, max_range, noise_std[0])\n",
    "    if hit:\n",
    "        hx, hy = hit\n",
    "        # true measurement\n",
    "        true_bearing = np.arctan2(hy-robot[1], hx-robot[0])\n",
    "        noisy_r = dist #+ np.random.normal(0, noise_std[0])\n",
    "        noisy_b = true_bearing + np.random.normal(0, noise_std[1])\n",
    "        mx = int(robot[0] + noisy_r * np.cos(noisy_b))\n",
    "        my = int(robot[1] + noisy_r * np.sin(noisy_b))\n",
    "        registered.append((mx, my, now))\n",
    "        pygame.draw.line(screen, (255,0,0), robot, (hx, hy), 2)\n",
    "\n",
    "    # Prune old landmarks\n",
    "    registered = [(x,y,t) for (x,y,t) in registered if now-t<=landmark_duration_ms]\n",
    "\n",
    "    # Right panel: Sensor-registered landmarks\n",
    "    # white background\n",
    "    pygame.draw.rect(screen, (0,0,0), (w, 0, w, h))\n",
    "    for x, y, _ in registered:\n",
    "        pygame.draw.circle(screen, (255,0,0), (x + w, y), 2)\n",
    "\n",
    "    pygame.display.flip()\n",
    "\n",
    "pygame.quit()\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b231645",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157a80f",
   "metadata": {},
   "source": [
    "##### üïπÔ∏è `Python example #6: Combined Interactive Motion & Observation Model`\n",
    "\n",
    "In this interactive demo, we control the robot with the keyboard arrows and see both motion and sensing in action:\n",
    "\n",
    "- <span style=\"color:#00703c\">**Left panel (Ground Truth)**</span>\n",
    "  - The blue circle shows the robot‚Äôs true pose as you drive it (no noise).  \n",
    "  - The blue line traces the ground truth trajectory behind the robot.\n",
    "\n",
    "- <span style=\"color:#00703c\">**Right panel (With Noise)**</span>\n",
    "  - The yellow line shows the noisy trajectory produced by the odometry-based motion model (`sample_motion_model_odometry`).  \n",
    "  - The red dots show the noisy range-bearing measurements (‚Äúlandmarks‚Äù) registered by the sensor model at each scan.\n",
    "\n",
    "<span style=\"color:#00703c\">**How it works**:</span>  \n",
    "1. <span style=\"color:#ffa500\">**Arrow keys**</span> issue motion commands (forward, turn left, turn right).  \n",
    "2. We update the <span style=\"color:#ffa500\">**ground truth pose**</span> exactly (no noise) and record its trajectory.  \n",
    "3. We update the <span style=\"color:#ffa500\">**predicted pose**</span> by applying the same command through the noisy motion model, and record its trajectory.  \n",
    "4. The sensor beam rotates at 10 Hz from the <span style=\"color:#ffa500\">**predicted pose**</span>, casting rays until it hits a wall pixel in the floor plan (max range in pixels).  \n",
    "5. We add Gaussian noise to both range and bearing, compute the noisy endpoint, and register it as a red landmark on the right panel.  \n",
    "\n",
    "Move the robot around with the arrow keys and watch how motion noise and sensor noise affect your map of the environment!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# --- Setup ---\n",
    "pygame.init()\n",
    "\n",
    "# Panel and window sizes\n",
    "PANEL_W, PANEL_H = 600, 400  # Adjust panel size as needed\n",
    "WINDOW_SIZE = (PANEL_W * 2, PANEL_H)\n",
    "screen = pygame.display.set_mode(WINDOW_SIZE)\n",
    "pygame.display.set_caption(\"Combined Motion & Sensor Demo\")\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Load & scale floor plan for left panel\n",
    "floor_plan_img = pygame.image.load('../figures/floor_plan.png') # Replace with any black-and-white floor plan image\n",
    "floor_plan = pygame.transform.smoothscale(floor_plan_img, (PANEL_W, PANEL_H))\n",
    "\n",
    "# Build occupancy map (True = wall) from scaled image\n",
    "arr = pygame.surfarray.array3d(floor_plan)\n",
    "wall_mask = np.all(arr < 128, axis=2)\n",
    "\n",
    "# --- State ---\n",
    "gt_pose    = np.array([PANEL_W//2, PANEL_H//2, 0.0])  # init pose\n",
    "pred_pose  = gt_pose.copy()                           # init noisy pose\n",
    "gt_traj    = [gt_pose.copy()]\n",
    "pred_traj  = [pred_pose.copy()]\n",
    "landmarks  = []    # list of (x, y, timestamp_ms)\n",
    "\n",
    "# --- Parameters ---\n",
    "motion_noise  = [0.02, 1.0, 0.02]       # [rot1, trans, rot2] noise\n",
    "TRANS_STEP    = 10                     # pixels per UP\n",
    "ROT_STEP      = np.deg2rad(15)         # radians per LEFT/RIGHT\n",
    "\n",
    "beam_speed    = 2 * np.pi * 10         # rad/s (10 rev/s)\n",
    "beam_angle    = 0.0\n",
    "max_range_px  = 200\n",
    "\n",
    "# SCAN_EVENT at 10 Hz\n",
    "SCAN_EVENT = pygame.USEREVENT + 1\n",
    "pygame.time.set_timer(SCAN_EVENT, 100)\n",
    "\n",
    "# Precompute angles for full 360¬∞ sweep\n",
    "NUM_BEAMS = 180\n",
    "angles = np.linspace(0, 2*np.pi, NUM_BEAMS, endpoint=False)\n",
    "\n",
    "# --- Main Loop ---\n",
    "running = True\n",
    "while running:\n",
    "    dt = clock.tick(60) / 1000.0\n",
    "    beam_angle = (beam_angle + beam_speed * dt) % (2 * np.pi)\n",
    "    now = pygame.time.get_ticks()\n",
    "\n",
    "    for ev in pygame.event.get():\n",
    "        if ev.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "        # motion commands\n",
    "        elif ev.type == pygame.KEYDOWN:\n",
    "            if ev.key == pygame.K_UP:\n",
    "                cmd = [0.0, TRANS_STEP, 0.0]\n",
    "            elif ev.key == pygame.K_LEFT:\n",
    "                cmd = [-ROT_STEP, 0.0, 0.0]\n",
    "            elif ev.key == pygame.K_RIGHT:\n",
    "                cmd = [ROT_STEP, 0.0, 0.0]\n",
    "            else:\n",
    "                cmd = None\n",
    "\n",
    "            if cmd is not None:\n",
    "                gt_pose = motion_model_odometry(cmd, gt_pose)\n",
    "                gt_traj.append(gt_pose.copy())\n",
    "                pred_pose = motion_model_odometry(cmd, pred_pose, motion_noise)\n",
    "                pred_traj.append(pred_pose.copy())\n",
    "\n",
    "        # full 360¬∞ sensor scan at 10 Hz\n",
    "        elif ev.type == SCAN_EVENT:\n",
    "            for a in angles:\n",
    "                hit, dist = cast_beam(gt_pose[:2], a, max_range_px, noise_std[0])                    \n",
    "                if hit:\n",
    "                    # Add noise to the measurement\n",
    "                    # dist += np.random.normal(0, noise_std[0])\n",
    "                    a += np.random.normal(0, noise_std[1])\n",
    "                    hx, hy = hit\n",
    "                    phi = a - gt_pose[2]                              # true bearing\n",
    "                    # project measurement from predicted pose\n",
    "                    mx = int(pred_pose[0] + dist * np.cos(pred_pose[2] + phi))\n",
    "                    my = int(pred_pose[1] + dist * np.sin(pred_pose[2] + phi))\n",
    "                    landmarks.append((mx, my, now))\n",
    "\n",
    "    # prune landmarks older than 10s\n",
    "    landmarks = [(x,y,t) for (x,y,t) in landmarks if now - t <= 1000000]\n",
    "\n",
    "    # --- Left Panel: Map + GT Trajectory + Rotating Beam ---\n",
    "    screen.fill((30,30,30), (0,0,PANEL_W,PANEL_H))\n",
    "    screen.blit(floor_plan, (0,0))\n",
    "    pygame.draw.rect(screen, (200,200,200), (0,0,PANEL_W,PANEL_H), 2)\n",
    "\n",
    "    # draw GT trajectory\n",
    "    for i in range(1, len(gt_traj)):\n",
    "        x0,y0 = gt_traj[i-1][:2]; x1,y1 = gt_traj[i][:2]\n",
    "        pygame.draw.line(screen, (0,150,255), (x0,y0), (x1,y1), 2)\n",
    "\n",
    "    # draw GT robot\n",
    "    gx,gy,gth = gt_pose\n",
    "    pts = [\n",
    "        (gx + 15*np.cos(gth),     gy + 15*np.sin(gth)),\n",
    "        (gx + 10*np.cos(gth+2.5), gy + 10*np.sin(gth+2.5)),\n",
    "        (gx + 10*np.cos(gth-2.5), gy + 10*np.sin(gth-2.5))\n",
    "    ]\n",
    "    pygame.draw.polygon(screen, (0,150,255), pts)\n",
    "\n",
    "    # draw rotating beam\n",
    "    bx, by = gt_pose[:2]\n",
    "    # compute endpoint even if no hit\n",
    "    hit, dist = cast_beam(gt_pose[:2], beam_angle, max_range_px)\n",
    "    if hit:\n",
    "        ex, ey = hit\n",
    "    else:\n",
    "        ex = int(bx + max_range_px * np.cos(beam_angle))\n",
    "        ey = int(by + max_range_px * np.sin(beam_angle))\n",
    "    pygame.draw.line(screen, (255,0,0), (int(bx), int(by)), (ex, ey), 2)\n",
    "\n",
    "    # --- Right Panel: Noisy Trajectory + Projected Landmarks ---\n",
    "    ox = PANEL_W\n",
    "    screen.fill((255,255,255), (ox,0,PANEL_W,PANEL_H))\n",
    "    pygame.draw.rect(screen, (200,200,200), (ox,0,PANEL_W,PANEL_H), 2)\n",
    "\n",
    "    # predicted trajectory\n",
    "    for i in range(1, len(pred_traj)):\n",
    "        x0,y0 = pred_traj[i-1][:2]; x1,y1 = pred_traj[i][:2]\n",
    "        pygame.draw.line(screen, (200,200,60), (x0+ox,y0), (x1+ox,y1), 2)\n",
    "\n",
    "    # predicted robot\n",
    "    px,py,_ = pred_pose\n",
    "    pygame.draw.circle(screen, (200,200,60), (int(px)+ox, int(py)), 8)\n",
    "\n",
    "    # projected landmarks\n",
    "    for x,y,_ in landmarks:\n",
    "        pygame.draw.circle(screen, (255,0,0), (x+ox, y), 2)\n",
    "\n",
    "    pygame.display.flip()\n",
    "\n",
    "pygame.quit()\n",
    "sys.exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73a4d9",
   "metadata": {},
   "source": [
    "#### ‚úîÔ∏è <span style=\"color:#a4d4a3\">Conclusion</span>\n",
    "\n",
    "In this final example, we combine the motion and sensor models to show how a robot builds a map of its environment while navigating. As you control the robot, both motion and measurement noise affect its understanding of the world. The landmarks it observes are uncertain, and its position estimate drifts over time. This highlights why we need a <span style=\"color:#ffa500\">**filtering method**</span> ‚Äî like the Extended Kalman Filter (EKF) or particle filter ‚Äî to <span style=\"color:#ffa500\">**predict**</span> the robot's state and <span style=\"color:#ffa500\">**correct**</span> it using noisy observations. Without such a filter, the robot's map and localization would quickly become unreliable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd360c8f",
   "metadata": {},
   "source": [
    "### üìö <span style=\"color:#a4d4a3\">**Reading Material**</span>\n",
    "\n",
    "**On Motion and Observation Models**\n",
    "- Thrun et al.: *\"Probabilistic Robotics\"*, **Chapter 5 & 6**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
